{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bergi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip install mlforecast\n",
    "!pip install statsforecast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from prophet import Prophet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_length = [96, 192, 336, 720]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from mlforecast import MLForecast\n",
    "from utilsforecast.losses import rmse, mae\n",
    "\n",
    "# List of datasets\n",
    "datasets = ['electricity.csv', 'traffic.csv', 'weather.csv', 'ETTh1.csv', 'ETTm1.csv', 'exchange_rate.csv']\n",
    "horizons = [96, 192, 336, 720] \n",
    "frequency_mapping = {\n",
    "    'electricity.csv': 'H',     \n",
    "    'traffic.csv': 'H',         \n",
    "    'exchange_rate.csv': 'D',   \n",
    "    'weather.csv': '10min',        \n",
    "    'ETTh1.csv': 'H',          \n",
    "    'ETTm1.csv': '15min'            \n",
    "}\n",
    "\n",
    "def process_dataset(file_name, horizons):\n",
    "    data = pd.read_csv(file_name)\n",
    "    if file_name == 'electricity.csv' or file_name == 'traffic.csv':\n",
    "      data = data.iloc[-5000:, :]\n",
    "    data['ds'] = pd.to_datetime(data['date'])\n",
    "    data.drop(columns=['date'], inplace=True)\n",
    "    \n",
    "    numeric_columns = data.columns.difference(['ds'])\n",
    "    data[numeric_columns] = MinMaxScaler().fit_transform(data[numeric_columns])\n",
    "    \n",
    "    df_melted = data.melt(id_vars=['ds'], var_name='unique_id', value_name='y')\n",
    "    \n",
    "    freq = frequency_mapping.get(file_name, 'D')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        models = [GradientBoostingRegressor()]\n",
    "        \n",
    "        mlf = MLForecast(\n",
    "            models=models,\n",
    "            freq=freq,\n",
    "            date_features=['dayofweek', 'year', 'month'],\n",
    "            lags = [2, 7],\n",
    "        )\n",
    "        \n",
    "        crossvalidation_df = mlf.cross_validation(\n",
    "            df=df_melted,\n",
    "            h=horizon,\n",
    "            n_windows= 10,\n",
    "        )\n",
    "        \n",
    "        crossvalidation_df['id_cutoff'] = crossvalidation_df['unique_id'] + '_' + crossvalidation_df['cutoff'].astype(str)\n",
    "        \n",
    "        cv_rmse = rmse(crossvalidation_df, models=['GradientBoostingRegressor'], id_col='id_cutoff')['GradientBoostingRegressor'].mean()\n",
    "        cv_mae = mae(crossvalidation_df, models=['GradientBoostingRegressor'], id_col='id_cutoff')['GradientBoostingRegressor'].mean()\n",
    "        \n",
    "        result = {\n",
    "            'dataset': file_name,\n",
    "            'horizon': horizon,\n",
    "            'rmse': cv_rmse,\n",
    "            'mae': cv_mae,\n",
    "            'std_dev_rmse': crossvalidation_df.groupby('id_cutoff').apply(lambda x: np.std(x['GradientBoostingRegressor'])).mean(),\n",
    "            'std_dev_mae': crossvalidation_df.groupby('id_cutoff').apply(lambda x: np.std(x['GradientBoostingRegressor'])).mean(),\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process all datasets and store results\n",
    "all_results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    results = process_dataset(dataset, horizons)\n",
    "    all_results.extend(results)\n",
    "\n",
    "# Convert results to DataFrame \n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('results_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsforecast.models import AutoARIMA\n",
    "from statsforecast import StatsForecast\n",
    "from utilsforecast.losses import rmse, mae\n",
    "\n",
    "# List of datasets\n",
    "datasets = ['exchange_rate.csv', 'ETTh1.csv', 'ETTm1.csv', 'weather.csv', 'electricity.csv', 'traffic.csv']\n",
    "horizons = [96, 192, 336, 720] \n",
    "frequency_mapping = {\n",
    "    'electricity.csv': 'H',     \n",
    "    'traffic.csv': 'H',         \n",
    "    'exchange_rate.csv': 'D',   \n",
    "    'weather.csv': '10min',        \n",
    "    'ETTh1.csv': 'H',          \n",
    "    'ETTm1.csv': '15min'            \n",
    "}\n",
    "\n",
    "def process_dataset(file_name, horizons):\n",
    "    data = pd.read_csv(file_name)\n",
    "    data = data.iloc[-5000:, :]\n",
    "    data['ds'] = pd.to_datetime(data['date'])\n",
    "    data.drop(columns=['date'], inplace=True)\n",
    "    \n",
    "    numeric_columns = data.columns.difference(['ds'])\n",
    "    data[numeric_columns] = MinMaxScaler().fit_transform(data[numeric_columns])\n",
    "    \n",
    "    df_melted = data.melt(id_vars=['ds'], var_name='unique_id', value_name='y')\n",
    "    \n",
    "    freq = frequency_mapping.get(file_name, 'D')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        models = [AutoARIMA(),]\n",
    "        \n",
    "        sf = StatsForecast(\n",
    "            models=models,\n",
    "            freq=freq,\n",
    "            df = df_melted,\n",
    "        )\n",
    "        \n",
    "        crossvalidation_df = sf.cross_validation(\n",
    "            df=df_melted,\n",
    "            h=horizon,\n",
    "            step_size = horizon,\n",
    "            n_windows= 5,\n",
    "        )\n",
    "        \n",
    "        crossvalidation_df['id_cutoff'] = crossvalidation_df.index + '_' + crossvalidation_df['cutoff'].astype(str)\n",
    "        \n",
    "        cv_rmse = rmse(crossvalidation_df, models=['AutoARIMA'], id_col='id_cutoff')['AutoARIMA'].mean()\n",
    "        cv_mae = mae(crossvalidation_df, models=['AutoARIMA'], id_col='id_cutoff')['AutoARIMA'].mean()\n",
    "        \n",
    "        result = {\n",
    "            'dataset': file_name,\n",
    "            'horizon': horizon,\n",
    "            'rmse': cv_rmse,\n",
    "            'mae': cv_mae,\n",
    "            'std_dev_rmse': crossvalidation_df.groupby('id_cutoff').apply(lambda x: np.std(x['AutoARIMA'])).mean(),\n",
    "            'std_dev_mae': crossvalidation_df.groupby('id_cutoff').apply(lambda x: np.std(x['AutoARIMA'])).mean(),\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process all datasets and store results\n",
    "all_results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    results = process_dataset(dataset, horizons)\n",
    "    all_results.extend(results)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('results_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsforecast.models import Naive\n",
    "from statsforecast import StatsForecast\n",
    "from utilsforecast.losses import rmse, mae\n",
    "\n",
    "# List of datasets\n",
    "datasets = ['exchange_rate.csv', 'ETTh1.csv', 'ETTm1.csv',  'electricity.csv', 'traffic.csv', 'weather.csv']\n",
    "horizons = [96, 192, 336, 720] \n",
    "frequency_mapping = {\n",
    "    'electricity.csv': 'H',     \n",
    "    'traffic.csv': 'H',         \n",
    "    'exchange_rate.csv': 'D',   \n",
    "    'weather.csv': '10min',        \n",
    "    'ETTh1.csv': 'H',          \n",
    "    'ETTm1.csv': '15min'            \n",
    "}\n",
    "\n",
    "def process_dataset(file_name, horizons):\n",
    "    data = pd.read_csv(file_name)\n",
    "    if file_name == 'electricity.csv' or file_name == 'traffic.csv':\n",
    "      data = data.iloc[-5000:, :]\n",
    "    data['ds'] = pd.to_datetime(data['date'])\n",
    "    data.drop(columns=['date'], inplace=True)\n",
    "    \n",
    "    numeric_columns = data.columns.difference(['ds'])\n",
    "    data[numeric_columns] = MinMaxScaler().fit_transform(data[numeric_columns])\n",
    "    \n",
    "    df_melted = data.melt(id_vars=['ds'], var_name='unique_id', value_name='y')\n",
    "    \n",
    "    freq = frequency_mapping.get(file_name, 'D')\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for horizon in horizons:\n",
    "        models = [Naive(),]\n",
    "        \n",
    "        sf = StatsForecast(\n",
    "            models=models,\n",
    "            freq=freq,\n",
    "            df = df_melted,\n",
    "        )\n",
    "        \n",
    "        crossvalidation_df = sf.cross_validation(\n",
    "            df=df_melted,\n",
    "            h=horizon,\n",
    "            step_size = horizon,\n",
    "            n_windows= 10,\n",
    "        )\n",
    "        \n",
    "        crossvalidation_df['id_cutoff'] = crossvalidation_df.index + '_' + crossvalidation_df['cutoff'].astype(str)\n",
    "        \n",
    "        cv_rmse = rmse(crossvalidation_df, models=['Naive'], id_col='id_cutoff')['Naive'].mean()\n",
    "        cv_mae = mae(crossvalidation_df, models=['Naive'], id_col='id_cutoff')['Naive'].mean()\n",
    "        \n",
    "        result = {\n",
    "            'dataset': file_name,\n",
    "            'horizon': horizon,\n",
    "            'rmse': cv_rmse,\n",
    "            'mae': cv_mae,\n",
    "            'std_dev_rmse': crossvalidation_df.groupby('id_cutoff').apply(lambda x: np.std(x['Naive'])).mean(),\n",
    "            'std_dev_mae': crossvalidation_df.groupby('id_cutoff').apply(lambda x: np.std(x['Naive'])).mean(),\n",
    "        }\n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Process all datasets and store results\n",
    "all_results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    results = process_dataset(dataset, horizons)\n",
    "    all_results.extend(results)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(results_df)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('results_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prophet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Suppress debug and info messages from Prophet\n",
    "logging.getLogger('prophet').setLevel(logging.WARNING)\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "\n",
    "# datasets = [('electricity','15T'), ('traffic','H'), ('exchange_rate','D'),('weather','10T'), ('ETTh1','H'), ('ETTm1','15T')]\n",
    "datasets = [('ETTm1','15T')]\n",
    "\n",
    "for dataset,fr in datasets:\n",
    "    df = pd.read_csv(f'/content/drive/MyDrive/sl project/dataset/{dataset}.csv', index_col='date', parse_dates=True)\n",
    "\n",
    "    # Min-max normalizer\n",
    "    scaler = MinMaxScaler()\n",
    "    df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "    evaluation_results = pd.DataFrame(index=pred_length, columns=['mae', 'rmse'])\n",
    "\n",
    "    for horizon in pred_length:\n",
    "        tot_mae = 0\n",
    "        tot_rmse = 0\n",
    "\n",
    "        for col in df.columns:\n",
    "            ts = df[col].reset_index()\n",
    "            ts.columns = ['ds', 'y']\n",
    "\n",
    "            train_size = len(ts) - horizon\n",
    "            train_ts = ts.iloc[:train_size]\n",
    "            test_ts = ts.iloc[train_size:train_size + horizon]\n",
    "\n",
    "            # Fit Prophet model\n",
    "            model = Prophet()\n",
    "            model.fit(train_ts)\n",
    "\n",
    "            # Make future dataframe\n",
    "            future = model.make_future_dataframe(periods=horizon, freq=fr)\n",
    "            forecast = model.predict(future)\n",
    "\n",
    "            # Extract the predictions\n",
    "            y_pred = forecast['yhat'].iloc[-horizon:].values\n",
    "            y_test = test_ts['y'].values\n",
    "\n",
    "            # Compute MAE and RMSE\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "            tot_mae += mae\n",
    "            tot_rmse += rmse\n",
    "\n",
    "        evaluation_results.loc[horizon, 'mae'] = tot_mae / len(df.columns)\n",
    "        evaluation_results.loc[horizon, 'rmse'] = tot_rmse / len(df.columns)\n",
    "\n",
    "    print(dataset)\n",
    "    print(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTSF Linear \n",
    "\n",
    "Implementation of the Linear Long Time Series Forcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Settings seed for reproducibility \n",
    "np.random.seed(42)\n",
    "\n",
    "dataset_path = '/kaggle/input/sl-project/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of the model\n",
    "lookback_window = 96\n",
    "prediction_lengths = [96, 192, 336, 720]\n",
    "\n",
    "# All the datasets\n",
    "datasets = ['weather', 'exchange_rate', 'traffic', 'electricity', 'ETTh1', 'ETTm1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LTSF-Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the model as a class that inherits from nn.Module\n",
    "# Taken from https://github.com/cure-lab/LTSF-Linear/\n",
    "\n",
    "class LTSFLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, loopback_window, prediction_length):\n",
    "        super(LTSFLinear, self).__init__()\n",
    "        self.loopback_window = loopback_window\n",
    "        self.prediction_length = prediction_length\n",
    "\n",
    "        # The core of the model, a simple linear layer\n",
    "        self.Linear = nn.Linear(self.loopback_window, self.prediction_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.Linear(x.permute(0,2,1)).permute(0,2,1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert the original dataframe to numpy arrays for pythorch model training\n",
    "\n",
    "# Basically each element of X contains the sequence of data points of length lookback_window\n",
    "# and the corresponding elements of y contains the sequence of data points of length prediction_length, \n",
    "# i.e. the data points to be predicted\n",
    "\n",
    "\n",
    "def build_designMatrixAndPrediction(data, lookback_window, prediction_length):\n",
    "\n",
    "    # Each row of X contains the sequence of data points of length lookback_window\n",
    "    # Each row of y contains the sequence of data points of length prediction_length\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data)-lookback_window-prediction_length+1):\n",
    "        X.append(data[i : i+lookback_window])\n",
    "        y.append(data[i+lookback_window : i+lookback_window+prediction_length])\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model with MSE Loss and Adam optimizer\n",
    "# This is a modified version of the function provided in the github repository\n",
    "\n",
    "def train_model(model, X_train, y_train, epochs, batch_size):\n",
    "\n",
    "    # MSE Loss\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            batch_X = X_train[i:i+batch_size]\n",
    "            batch_y = y_train[i:i+batch_size]\n",
    "            \n",
    "            outputs = model(batch_X)\n",
    "\n",
    "            # Evaluate MSE loss and backpropagate\n",
    "            loss = mse(outputs, batch_y)            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Log the loss every 10 epochs\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}] - Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciton to test the model\n",
    "#  It outputts the MAE and RMSE of the model on the test data given as input\n",
    "\n",
    "def test_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test)\n",
    "\n",
    "    #  Reshaping the data to the original shape\n",
    "    # before evaluating the metrics\n",
    "    y_test_inv = y_test.reshape(-1, y_test.shape[-1])\n",
    "    y_pred_inv = y_pred.reshape(-1, y_pred.shape[-1])\n",
    "    \n",
    "    mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "    \n",
    "    return mae, rmse, y_test_inv, y_pred_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate TSs\n",
    "\n",
    "We first train the model only on the dataset that have univariate Time Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate TS datasets\n",
    "\n",
    "datasets_uni = ['exchange_rate', 'traffic', 'electricity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dataset, train the model for each prediction length\n",
    "# and store the results in a dictionary that later will be converted into a datatframe \n",
    "\n",
    "results = {}\n",
    "\n",
    "for dataset in datasets_uni:\n",
    "\n",
    "    print(\"\\nDataset: \", dataset)\n",
    "\n",
    "    df = pd.read_csv(dataset_path + dataset + '.csv', index_col='date', parse_dates=True)\n",
    "\n",
    "    # Scaling all the columns in the range[0, 1]\n",
    "    scaler = MinMaxScaler()\n",
    "    df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "    \n",
    "    results[dataset] = {}\n",
    "\n",
    "    for pred_len in prediction_lengths:\n",
    "\n",
    "        print(70 * \"-\")\n",
    "        print(f\"\\nTraining LTSF-Linear for prediction length: {pred_len}\")\n",
    "        \n",
    "        # Converting the dataframe to numpy arrays\n",
    "        # such that the moel can be trained on it\n",
    "        X, y = build_designMatrixAndPrediction(df, lookback_window, pred_len)\n",
    "        \n",
    "        # Splitting the data in 80% train and 20% test\n",
    "        train_size = int(len(X) * 0.8)\n",
    "        X_train, X_test = torch.FloatTensor(X[:train_size]), torch.FloatTensor(X[train_size:])\n",
    "        y_train, y_test = torch.FloatTensor(y[:train_size]), torch.FloatTensor(y[train_size:])\n",
    "        \n",
    "        # Initializing and training the model\n",
    "        modelLSTF = LTSFLinear(lookback_window, prediction_length = pred_len)\n",
    "        \n",
    "        train_model(modelLSTF, X_train, y_train, epochs = 100, batch_size = 32)\n",
    "        \n",
    "        # Testing the model and saving its metrics\n",
    "        mae, rmse, y_test_inv, y_pred_inv = test_model(modelLSTF, X_test, y_test, scaler)\n",
    "        \n",
    "        results[dataset][pred_len] = {\n",
    "            'MAE': mae,\n",
    "            'RMSE': rmse\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th>Prediction Length</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">exchange_rate</th>\n",
       "      <th>96</th>\n",
       "      <td>0.044368</td>\n",
       "      <td>0.059844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.073013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.079542</td>\n",
       "      <td>0.105306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.185081</td>\n",
       "      <td>0.224984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">traffic</th>\n",
       "      <th>96</th>\n",
       "      <td>0.040704</td>\n",
       "      <td>0.078620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.037695</td>\n",
       "      <td>0.075344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.039054</td>\n",
       "      <td>0.075822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">electricity</th>\n",
       "      <th>96</th>\n",
       "      <td>0.054440</td>\n",
       "      <td>0.080014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.058545</td>\n",
       "      <td>0.082661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.056093</td>\n",
       "      <td>0.081940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.058820</td>\n",
       "      <td>0.084733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      MAE      RMSE\n",
       "Dataset       Prediction Length                    \n",
       "exchange_rate 96                 0.044368  0.059844\n",
       "              192                0.054688  0.073013\n",
       "              336                0.079542  0.105306\n",
       "              720                0.185081  0.224984\n",
       "traffic       96                 0.040704  0.078620\n",
       "              192                0.037695  0.075344\n",
       "              336                0.039054  0.075822\n",
       "electricity   96                 0.054440  0.080014\n",
       "              192                0.058545  0.082661\n",
       "              336                0.056093  0.081940\n",
       "              720                0.058820  0.084733"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating a DataFrame from the results dictionary\n",
    "multi_index = pd.MultiIndex.from_tuples(\n",
    "    [(dataset, pred_len) for dataset in results for pred_len in results[dataset]],\n",
    "    names=['Dataset', 'Prediction Length']\n",
    ")\n",
    "\n",
    "df_results = pd.DataFrame(\n",
    "    [(results[dataset][pred_len]['MAE'], results[dataset][pred_len]['RMSE'])\n",
    "     for dataset in results for pred_len in results[dataset]],\n",
    "    index=multi_index,\n",
    "    columns=['MAE', 'RMSE']\n",
    ")\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing df_results to a csv file\n",
    "df_results.to_csv('results_uni.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate TSs\n",
    "\n",
    "We first train the model only on the dataset that have multivariate Time Series.\n",
    "\n",
    "Each file is a single Time Serie that have multiple feature to  be predicted.\n",
    "\n",
    "In the same way that they do in the article, we will predict each feature separately using the LTSF-Linear model.\n",
    "\n",
    "Since the datasets are too long, in order to not let the computation time exploding for high prediction windows we take only the last 10000 datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate TS datasets\n",
    "\n",
    "datasets_multi = ['weather', 'ETTh1', 'ETTm1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each dataset, train the model for each prediction length\n",
    "# and store the results in a dictionary that later will be converted into a datatframe \n",
    "\n",
    "results_multi = {}\n",
    "\n",
    "for dataset in datasets_multi:\n",
    "\n",
    "    print(\"\\nDataset: \", dataset)\n",
    "\n",
    "    df = pd.read_csv(dataset_path + dataset + '.csv', index_col='date', parse_dates=True)\n",
    "    \n",
    "    # Taking the last 10000 datapoints of the Time Serie\n",
    "    df = df.iloc[-10000:,:]\n",
    "\n",
    "    # Scaling all the columns in the range[0, 1]\n",
    "    scaler = MinMaxScaler()\n",
    "    df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "    \n",
    "    results_multi[dataset] = {}\n",
    "\n",
    "    for pred_len in prediction_lengths:\n",
    "\n",
    "        print(70 * \"-\")\n",
    "        print(f\"\\nTraining LTSF-Linear for prediction length: {pred_len}\")\n",
    "\n",
    "\n",
    "        # For each prediction length, we train the model for each column of the dataset\n",
    "        # And then we take the average of the MAE and RMSE of all the columns\n",
    "        maes = []\n",
    "        rmses = []\n",
    "\n",
    "        for col in df.columns:\n",
    "            print(70 * \"-\")\n",
    "            print(f\"\\nTraining for column: {col}\")            \n",
    "            \n",
    "            # Converting the dataframe to numpy arrays\n",
    "            # such that the moel can be trained on it\n",
    "            X, y = build_designMatrixAndPrediction(pd.DataFrame(df[col]), lookback_window, pred_len)\n",
    "            \n",
    "            # Split the data in 80% train and 20% test\n",
    "            train_size = int(len(X) * 0.8)\n",
    "            X_train, X_test = torch.FloatTensor(X[:train_size]), torch.FloatTensor(X[train_size:])\n",
    "            y_train, y_test = torch.FloatTensor(y[:train_size]), torch.FloatTensor(y[train_size:])\n",
    "            \n",
    "            # Initializing and training the model\n",
    "            modelLSTF = LTSFLinear(lookback_window, prediction_length = pred_len)\n",
    "            \n",
    "            train_model(modelLSTF, X_train, y_train, epochs = 50, batch_size = 32)\n",
    "            \n",
    "            # Testing the model and saving its metrics\n",
    "            mae, rmse, y_test_inv, y_pred_inv = test_model(modelLSTF, X_test, y_test, scaler)\n",
    "            maes.append(mae)\n",
    "            rmses.append(rmse)\n",
    "        \n",
    "        # Saving the average of the MAE and RMSE of all the columns\n",
    "        # In the dictionary\n",
    "        results_multi[dataset][pred_len] = {\n",
    "            'MAE': np.mean(maes),\n",
    "            'RMSE': np.mean(rmses)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th>Prediction Length</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">weather</th>\n",
       "      <th>96</th>\n",
       "      <td>0.089227</td>\n",
       "      <td>0.120001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.108004</td>\n",
       "      <td>0.144456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.143208</td>\n",
       "      <td>0.183607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.200576</td>\n",
       "      <td>0.245031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">ETTh1</th>\n",
       "      <th>96</th>\n",
       "      <td>0.076808</td>\n",
       "      <td>0.104071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.096826</td>\n",
       "      <td>0.125046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.109616</td>\n",
       "      <td>0.139671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.120480</td>\n",
       "      <td>0.155150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">ETTm1</th>\n",
       "      <th>96</th>\n",
       "      <td>0.073092</td>\n",
       "      <td>0.096906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.083562</td>\n",
       "      <td>0.107376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.087728</td>\n",
       "      <td>0.111352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.095922</td>\n",
       "      <td>0.121720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                MAE      RMSE\n",
       "Dataset Prediction Length                    \n",
       "weather 96                 0.089227  0.120001\n",
       "        192                0.108004  0.144456\n",
       "        336                0.143208  0.183607\n",
       "        720                0.200576  0.245031\n",
       "ETTh1   96                 0.076808  0.104071\n",
       "        192                0.096826  0.125046\n",
       "        336                0.109616  0.139671\n",
       "        720                0.120480  0.155150\n",
       "ETTm1   96                 0.073092  0.096906\n",
       "        192                0.083562  0.107376\n",
       "        336                0.087728  0.111352\n",
       "        720                0.095922  0.121720"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating a DataFrame from the results_multi dictionary\n",
    "multi_index = pd.MultiIndex.from_tuples(\n",
    "    [(dataset, pred_len) for dataset in results_multi for pred_len in results_multi[dataset]],\n",
    "    names=['Dataset', 'Prediction Length']\n",
    ")\n",
    "\n",
    "df_results_multi = pd.DataFrame(\n",
    "    [(results_multi[dataset][pred_len]['MAE'], results_multi[dataset][pred_len]['RMSE'])\n",
    "     for dataset in results_multi for pred_len in results_multi[dataset]],\n",
    "    index=multi_index,\n",
    "    columns=['MAE', 'RMSE']\n",
    ")\n",
    "\n",
    "df_results_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing df_results_multi to a csv file\n",
    "df_results_multi.to_csv('results_multi.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally merge the two dataframes with the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th>Prediction Length</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">electricity</th>\n",
       "      <th>96</th>\n",
       "      <td>0.054440</td>\n",
       "      <td>0.080014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.058545</td>\n",
       "      <td>0.082661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.056093</td>\n",
       "      <td>0.081940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.058820</td>\n",
       "      <td>0.084733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">exchange_rate</th>\n",
       "      <th>96</th>\n",
       "      <td>0.044368</td>\n",
       "      <td>0.059844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.073013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.079542</td>\n",
       "      <td>0.105306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.185081</td>\n",
       "      <td>0.224984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">traffic</th>\n",
       "      <th>96</th>\n",
       "      <td>0.040704</td>\n",
       "      <td>0.078620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.037695</td>\n",
       "      <td>0.075344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.039054</td>\n",
       "      <td>0.075822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.053482</td>\n",
       "      <td>0.090758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">weather</th>\n",
       "      <th>96</th>\n",
       "      <td>0.089227</td>\n",
       "      <td>0.120001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.108004</td>\n",
       "      <td>0.144456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.143208</td>\n",
       "      <td>0.183607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.200576</td>\n",
       "      <td>0.245031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">ETTh1</th>\n",
       "      <th>96</th>\n",
       "      <td>0.076808</td>\n",
       "      <td>0.104071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.096826</td>\n",
       "      <td>0.125046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.109616</td>\n",
       "      <td>0.139671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.120480</td>\n",
       "      <td>0.155150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">ETTm1</th>\n",
       "      <th>96</th>\n",
       "      <td>0.073092</td>\n",
       "      <td>0.096906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.083562</td>\n",
       "      <td>0.107376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0.087728</td>\n",
       "      <td>0.111352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>0.095922</td>\n",
       "      <td>0.121720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      MAE      RMSE\n",
       "Dataset       Prediction Length                    \n",
       "electricity   96                 0.054440  0.080014\n",
       "              192                0.058545  0.082661\n",
       "              336                0.056093  0.081940\n",
       "              720                0.058820  0.084733\n",
       "exchange_rate 96                 0.044368  0.059844\n",
       "              192                0.054688  0.073013\n",
       "              336                0.079542  0.105306\n",
       "              720                0.185081  0.224984\n",
       "traffic       96                 0.040704  0.078620\n",
       "              192                0.037695  0.075344\n",
       "              336                0.039054  0.075822\n",
       "              720                0.053482  0.090758\n",
       "weather       96                 0.089227  0.120001\n",
       "              192                0.108004  0.144456\n",
       "              336                0.143208  0.183607\n",
       "              720                0.200576  0.245031\n",
       "ETTh1         96                 0.076808  0.104071\n",
       "              192                0.096826  0.125046\n",
       "              336                0.109616  0.139671\n",
       "              720                0.120480  0.155150\n",
       "ETTm1         96                 0.073092  0.096906\n",
       "              192                0.083562  0.107376\n",
       "              336                0.087728  0.111352\n",
       "              720                0.095922  0.121720"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_results_final = pd.concat([df_results, df_results_multi])\n",
    "df_results_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the results to file\n",
    "df_results_final.to_csv('results_LTSF_Linear.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FreTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from models import FreTS\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class Dataset_ETT_hour(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=None,\n",
    "                 features='S', data_path='ETTh1.csv',\n",
    "                 target='OT', scale=True, timeenc=0, freq='h', train_only=False):\n",
    "        if size == None:\n",
    "            self.seq_len = 24 * 4 * 4\n",
    "            self.label_len = 24 * 4\n",
    "            self.pred_len = 24 * 4\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "        # init\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path))\n",
    "\n",
    "        border1s = [0, 12 * 30 * 24 - self.seq_len, 12 * 30 * 24 + 4 * 30 * 24 - self.seq_len]\n",
    "        border2s = [12 * 30 * 24, 12 * 30 * 24 + 4 * 30 * 24, 12 * 30 * 24 + 8 * 30 * 24]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == 'S':\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        mms = MinMaxScaler(feature_range=(0, 1))\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            mms.fit(train_data.values)\n",
    "            data = mms.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        df_stamp = df_raw[['date']][border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            data_stamp = df_stamp.drop(['date'], 1).values\n",
    "        elif self.timeenc == 1:\n",
    "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        mms = MinMaxScaler(feature_range=(0, 1))\n",
    "        return mms.fit_transform(data.cpu())\n",
    "\n",
    "\n",
    "class Dataset_ETT_minute(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=None,\n",
    "                 features='S', data_path='ETTm1.csv',\n",
    "                 target='OT', scale=True, timeenc=0, freq='t'):\n",
    "        # size [seq_len, label_len, pred_len]\n",
    "        # info\n",
    "        if size == None:\n",
    "            self.seq_len = 24 * 4 * 4\n",
    "            self.label_len = 24 * 4\n",
    "            self.pred_len = 24 * 4\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "        # init\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path))\n",
    "\n",
    "        border1s = [0, 12 * 30 * 24 * 4 - self.seq_len, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4 - self.seq_len]\n",
    "        border2s = [12 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 4 * 30 * 24 * 4, 12 * 30 * 24 * 4 + 8 * 30 * 24 * 4]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == 'S':\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            self.scaler.fit(train_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        df_stamp = df_raw[['date']][border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute, 1)\n",
    "            df_stamp['minute'] = df_stamp.minute.map(lambda x: x // 15)\n",
    "            data_stamp = df_stamp.drop(['date'], 1).values\n",
    "        elif self.timeenc == 1:\n",
    "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "\n",
    "class Dataset_Covid(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=None,\n",
    "                 features='S', data_path='ETTh1.csv',\n",
    "                 target='OT', scale=True, timeenc=0, freq='h', train_only=False):\n",
    "        # size [seq_len, label_len, pred_len]\n",
    "        # info\n",
    "        if size == None:\n",
    "            self.seq_len = 24 * 4 * 4\n",
    "            self.label_len = 24 * 4\n",
    "            self.pred_len = 24 * 4\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "        # init\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "        self.train_only = train_only\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path))\n",
    "        df_raw = df_raw.dropna()\n",
    "\n",
    "        cols = list(df_raw.columns)\n",
    "        if self.features == 'S':\n",
    "            cols.remove(self.target)\n",
    "        cols.remove('date')\n",
    "\n",
    "        num_train = int(len(df_raw) * (0.6 if not self.train_only else 1))\n",
    "        num_test = int(len(df_raw) * 0.2)\n",
    "        num_vali = len(df_raw) - num_train - num_test\n",
    "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
    "        border2s = [num_train, num_train + num_vali, len(df_raw)]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            df_raw = df_raw[['date'] + cols]\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == 'S':\n",
    "            df_raw = df_raw[['date'] + cols + [self.target]]\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        ## min max scaler\n",
    "        mms = MinMaxScaler(feature_range=(0, 1))\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            mms.fit(train_data.values)\n",
    "            data = mms.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        df_stamp = df_raw[['date']][border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            data_stamp = df_stamp.drop(['date'], 1).values\n",
    "        elif self.timeenc == 1:\n",
    "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        mms = MinMaxScaler(feature_range=(0, 1))\n",
    "        return mms.fit_transform(data.cpu())\n",
    "\n",
    "#min max scaler\n",
    "class Dataset_Custom_(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=None,\n",
    "                 features='S', data_path='ETTh1.csv',\n",
    "                 target='OT', scale=True, timeenc=0, freq='h', train_only=False):\n",
    "        # size [seq_len, label_len, pred_len]\n",
    "        # info\n",
    "        if size == None:\n",
    "            self.seq_len = 24 * 4 * 4\n",
    "            self.label_len = 24 * 4\n",
    "            self.pred_len = 24 * 4\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "        # init\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "        self.train_only = train_only\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path))\n",
    "        df_raw = df_raw.dropna()\n",
    "\n",
    "        cols = list(df_raw.columns)\n",
    "        if self.features == 'S':\n",
    "            cols.remove(self.target)\n",
    "        cols.remove('date')\n",
    "\n",
    "        num_train = int(len(df_raw) * (0.7 if not self.train_only else 1))\n",
    "        num_test = int(len(df_raw) * 0.1)\n",
    "        num_vali = len(df_raw) - num_train - num_test\n",
    "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
    "        border2s = [num_train, num_train + num_vali, len(df_raw)]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            df_raw = df_raw[['date'] + cols]\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == 'S':\n",
    "            df_raw = df_raw[['date'] + cols + [self.target]]\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        ## min max scaler\n",
    "        mms = MinMaxScaler(feature_range=(0, 1))\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            mms.fit(train_data.values)\n",
    "            data = mms.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        df_stamp = df_raw[['date']][border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            data_stamp = df_stamp.drop(['date'], 1).values\n",
    "        elif self.timeenc == 1:\n",
    "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        mms = MinMaxScaler(feature_range=(0, 1))\n",
    "        return mms.fit_transform(data.cpu())\n",
    "        #return self.scaler.inverse_transform(data)\n",
    "\n",
    "class Dataset_Custom(Dataset):\n",
    "    def __init__(self, root_path, flag='train', size=None,\n",
    "                 features='S', data_path='ETTh1.csv',\n",
    "                 target='OT', scale=False, timeenc=0, freq='h', train_only=False):\n",
    "\n",
    "        if size == None:\n",
    "            self.seq_len = 24 * 4 * 4\n",
    "            self.label_len = 24 * 4\n",
    "            self.pred_len = 24 * 4\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "        # init\n",
    "        assert flag in ['train', 'test', 'val']\n",
    "        type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "        self.set_type = type_map[flag]\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "        self.train_only = train_only\n",
    "\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path))\n",
    "        df_raw = df_raw.dropna()\n",
    "\n",
    "        cols = list(df_raw.columns)\n",
    "        if self.features == 'S':\n",
    "            cols.remove(self.target)\n",
    "        cols.remove('date')\n",
    "\n",
    "        num_train = int(len(df_raw) * (0.7 if not self.train_only else 1))\n",
    "        num_test = int(len(df_raw) * 0.1)\n",
    "        num_vali = len(df_raw) - num_train - num_test\n",
    "        border1s = [0, num_train - self.seq_len, len(df_raw) - num_test - self.seq_len]\n",
    "        border2s = [num_train, num_train + num_vali, len(df_raw)]\n",
    "        border1 = border1s[self.set_type]\n",
    "        border2 = border2s[self.set_type]\n",
    "\n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            df_raw = df_raw[['date'] + cols]\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == 'S':\n",
    "            df_raw = df_raw[['date'] + cols + [self.target]]\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        if self.scale:\n",
    "            train_data = df_data[border1s[0]:border2s[0]]\n",
    "            self.scaler.fit(train_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        df_stamp = df_raw[['date']][border1:border2]\n",
    "        df_stamp['date'] = pd.to_datetime(df_stamp.date)\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            data_stamp = df_stamp.drop(['date'], 1).values\n",
    "        elif self.timeenc == 1:\n",
    "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "\n",
    "        self.data_x = data[border1:border2]\n",
    "        self.data_y = data[border1:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        seq_y = self.data_y[r_begin:r_end]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n",
    "\n",
    "class Dataset_Pred(Dataset):\n",
    "    def __init__(self, root_path, flag='pred', size=None,\n",
    "                 features='S', data_path='ETTh1.csv',\n",
    "                 target='OT', scale=True, inverse=False, timeenc=0, freq='15min', cols=None, train_only=False):\n",
    "\n",
    "        if size == None:\n",
    "            self.seq_len = 24 * 4 * 4\n",
    "            self.label_len = 24 * 4\n",
    "            self.pred_len = 24 * 4\n",
    "        else:\n",
    "            self.seq_len = size[0]\n",
    "            self.label_len = size[1]\n",
    "            self.pred_len = size[2]\n",
    "        # init\n",
    "        assert flag in ['pred']\n",
    "\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.scale = scale\n",
    "        self.inverse = inverse\n",
    "        self.timeenc = timeenc\n",
    "        self.freq = freq\n",
    "        self.cols = cols\n",
    "        self.root_path = root_path\n",
    "        self.data_path = data_path\n",
    "        self.__read_data__()\n",
    "\n",
    "    def __read_data__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        df_raw = pd.read_csv(os.path.join(self.root_path,\n",
    "                                          self.data_path))\n",
    "\n",
    "        if self.cols:\n",
    "            cols = self.cols.copy()\n",
    "        else:\n",
    "            cols = list(df_raw.columns)\n",
    "            self.cols = cols.copy()\n",
    "            cols.remove('date')\n",
    "        if self.features == 'S':\n",
    "            cols.remove(self.target)\n",
    "        border1 = len(df_raw) - self.seq_len\n",
    "        border2 = len(df_raw)\n",
    "\n",
    "        if self.features == 'M' or self.features == 'MS':\n",
    "            df_raw = df_raw[['date'] + cols]\n",
    "            cols_data = df_raw.columns[1:]\n",
    "            df_data = df_raw[cols_data]\n",
    "        elif self.features == 'S':\n",
    "            df_raw = df_raw[['date'] + cols + [self.target]]\n",
    "            df_data = df_raw[[self.target]]\n",
    "\n",
    "        if self.scale:\n",
    "            self.scaler.fit(df_data.values)\n",
    "            data = self.scaler.transform(df_data.values)\n",
    "        else:\n",
    "            data = df_data.values\n",
    "\n",
    "        tmp_stamp = df_raw[['date']][border1:border2]\n",
    "        tmp_stamp['date'] = pd.to_datetime(tmp_stamp.date)\n",
    "        pred_dates = pd.date_range(tmp_stamp.date.values[-1], periods=self.pred_len + 1, freq=self.freq)\n",
    "\n",
    "        df_stamp = pd.DataFrame(columns=['date'])\n",
    "        df_stamp.date = list(tmp_stamp.date.values) + list(pred_dates[1:])\n",
    "        self.future_dates = list(pred_dates[1:])\n",
    "        if self.timeenc == 0:\n",
    "            df_stamp['month'] = df_stamp.date.apply(lambda row: row.month, 1)\n",
    "            df_stamp['day'] = df_stamp.date.apply(lambda row: row.day, 1)\n",
    "            df_stamp['weekday'] = df_stamp.date.apply(lambda row: row.weekday(), 1)\n",
    "            df_stamp['hour'] = df_stamp.date.apply(lambda row: row.hour, 1)\n",
    "            df_stamp['minute'] = df_stamp.date.apply(lambda row: row.minute, 1)\n",
    "            df_stamp['minute'] = df_stamp.minute.map(lambda x: x // 15)\n",
    "            data_stamp = df_stamp.drop(['date'], 1).values\n",
    "        elif self.timeenc == 1:\n",
    "            data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "            data_stamp = data_stamp.transpose(1, 0)\n",
    "\n",
    "        self.data_x = data[border1:border2]\n",
    "        if self.inverse:\n",
    "            self.data_y = df_data.values[border1:border2]\n",
    "        else:\n",
    "            self.data_y = data[border1:border2]\n",
    "        self.data_stamp = data_stamp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        s_begin = index\n",
    "        s_end = s_begin + self.seq_len\n",
    "        r_begin = s_end - self.label_len\n",
    "        r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "        seq_x = self.data_x[s_begin:s_end]\n",
    "        if self.inverse:\n",
    "            seq_y = self.data_x[r_begin:r_begin + self.label_len]\n",
    "        else:\n",
    "            seq_y = self.data_y[r_begin:r_begin + self.label_len]\n",
    "        seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "        seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "        return seq_x, seq_y, seq_x_mark, seq_y_mark\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_x) - self.seq_len + 1\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return self.scaler.inverse_transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_dict = {\n",
    "    'ETTh1': Dataset_Custom_,#Dataset_ETT_hour,\n",
    "    'ETTm1': Dataset_Custom_,\n",
    "    'traffic': Dataset_Custom,\n",
    "    'electricity': Dataset_Custom_,\n",
    "    'exchange': Dataset_Custom_,\n",
    "    'weather': Dataset_Custom_,\n",
    "    'covid': Dataset_Covid,\n",
    "    'ECG': Dataset_Custom_,\n",
    "    'metr': Dataset_Custom_,\n",
    "}\n",
    "\n",
    "\n",
    "def data_provider(args, flag):\n",
    "    Data = data_dict[args.data]\n",
    "    timeenc = 0 if args.embed != 'timeF' else 1\n",
    "    train_only = args.train_only\n",
    "\n",
    "    if flag == 'test':\n",
    "        shuffle_flag = False\n",
    "        drop_last = True\n",
    "        batch_size = args.batch_size\n",
    "        freq = args.freq\n",
    "    elif flag == 'pred':\n",
    "        shuffle_flag = False\n",
    "        drop_last = False\n",
    "        batch_size = 1\n",
    "        freq = args.freq\n",
    "        Data = Dataset_Pred\n",
    "    else:\n",
    "        shuffle_flag = True\n",
    "        drop_last = True\n",
    "        batch_size = args.batch_size\n",
    "        freq = args.freq\n",
    "\n",
    "    data_set = Data(\n",
    "        root_path=args.root_path,\n",
    "        data_path=args.data_path,\n",
    "        flag=flag,\n",
    "        size=[args.seq_len, args.label_len, args.pred_len],\n",
    "        features=args.features,\n",
    "        target=args.target,\n",
    "        timeenc=timeenc,\n",
    "        freq=freq,\n",
    "        train_only=train_only\n",
    "    )\n",
    "    print(flag, len(data_set))\n",
    "    data_loader = DataLoader(\n",
    "        data_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle_flag,\n",
    "        num_workers=args.num_workers,\n",
    "        drop_last=drop_last)\n",
    "    return data_set, data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Exp_Basic(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.device = self._acquire_device()\n",
    "        self.model = self._build_model().to(self.device)\n",
    "\n",
    "    def _build_model(self):\n",
    "        raise NotImplementedError\n",
    "        return None\n",
    "\n",
    "    def _acquire_device(self):\n",
    "        if self.args.use_gpu:\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(\n",
    "                self.args.gpu) if not self.args.use_multi_gpu else self.args.devices\n",
    "            device = torch.device('cuda:{}'.format(self.args.gpu))\n",
    "            print('Use GPU: cuda:{}'.format(self.args.gpu))\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "            print('Use CPU')\n",
    "        return device\n",
    "\n",
    "    def _get_data(self):\n",
    "        pass\n",
    "\n",
    "    def vali(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def test(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch, args):\n",
    "    # lr = args.learning_rate * (0.2 ** (epoch // 2))\n",
    "    if args.lradj == 'type1':\n",
    "        lr_adjust = {epoch: args.learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "    elif args.lradj == 'type2':\n",
    "        lr_adjust = {\n",
    "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "        }\n",
    "    elif args.lradj == '3':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 10 else args.learning_rate*0.1}\n",
    "    elif args.lradj == '4':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 15 else args.learning_rate*0.1}\n",
    "    elif args.lradj == '5':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 25 else args.learning_rate*0.1}\n",
    "    elif args.lradj == '6':\n",
    "        lr_adjust = {epoch: args.learning_rate if epoch < 5 else args.learning_rate*0.1}\n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        print('Updating learning rate to {}'.format(lr))\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "class StandardScaler():\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def transform(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        return (data * self.std) + self.mean\n",
    "\n",
    "\n",
    "def visual(true, preds=None, name='./pic/test.pdf'):\n",
    "    \"\"\"\n",
    "    Results visualization\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(true, label='GroundTruth', linewidth=2)\n",
    "    if preds is not None:\n",
    "        plt.plot(preds, label='Prediction', linewidth=2)\n",
    "    plt.legend()\n",
    "    plt.savefig(name, bbox_inches='tight')\n",
    "\n",
    "def test_params_flop(model,x_shape):\n",
    "    \"\"\"\n",
    "    If you want to thest former's flop, you need to give default value to inputs in model.forward(), the following code can only pass one argument to forward()\n",
    "    \"\"\"\n",
    "    model_params = 0\n",
    "    for parameter in model.parameters():\n",
    "        model_params += parameter.numel()\n",
    "        print('INFO: Trainable parameter count: {:.2f}M'.format(model_params / 1000000.0))\n",
    "    from ptflops import get_model_complexity_info\n",
    "    with torch.cuda.device(0):\n",
    "        macs, params = get_model_complexity_info(model.cuda(), x_shape, as_strings=True, print_per_layer_stat=True)\n",
    "        # print('Flops:' + flops)\n",
    "        # print('Params:' + params)\n",
    "        print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n",
    "        print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n",
    "\n",
    "\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.tseries import offsets\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "\n",
    "class TimeFeature:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "class SecondOfMinute(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class MinuteOfHour(TimeFeature):\n",
    "    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "class HourOfDay(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfWeek(TimeFeature):\n",
    "    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfMonth(TimeFeature):\n",
    "    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "class DayOfYear(TimeFeature):\n",
    "    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "class MonthOfYear(TimeFeature):\n",
    "    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "class WeekOfYear(TimeFeature):\n",
    "    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "        return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "\n",
    "def time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "\n",
    "\n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def RSE(pred, true):\n",
    "    return np.sqrt(np.sum((true - pred) ** 2)) / np.sqrt(np.sum((true - true.mean()) ** 2))\n",
    "\n",
    "\n",
    "def CORR(pred, true):\n",
    "    u = ((true - true.mean(0)) * (pred - pred.mean(0))).sum(0)\n",
    "    d = np.sqrt(((true - true.mean(0)) ** 2 * (pred - pred.mean(0)) ** 2).sum(0))\n",
    "    d += 1e-12\n",
    "    return 0.01*(u / d).mean(-1)\n",
    "\n",
    "\n",
    "def MAE(pred, true):\n",
    "    return np.mean(np.abs(pred - true))\n",
    "\n",
    "\n",
    "def MSE(pred, true):\n",
    "    return np.mean((pred - true) ** 2)\n",
    "\n",
    "\n",
    "def RMSE(pred, true):\n",
    "    return np.sqrt(MSE(pred, true))\n",
    "\n",
    "\n",
    "def MAPE(pred, true):\n",
    "    return np.mean(np.abs((pred - true) / true))\n",
    "\n",
    "\n",
    "def MSPE(pred, true):\n",
    "    return np.mean(np.square((pred - true) / true))\n",
    "\n",
    "\n",
    "def metric(pred, true):\n",
    "    mae = MAE(pred, true)\n",
    "    mse = MSE(pred, true)\n",
    "    rmse = RMSE(pred, true)\n",
    "    mape = MAPE(pred, true)\n",
    "    mspe = MSPE(pred, true)\n",
    "    rse = RSE(pred, true)\n",
    "    corr = CORR(pred, true)\n",
    "\n",
    "    return mae, mse, rmse, mape, mspe, rse, corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exp Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Exp_Main(Exp_Basic):\n",
    "    def __init__(self, args):\n",
    "        super(Exp_Main, self).__init__(args)\n",
    "\n",
    "    def _build_model(self):\n",
    "        model_dict = {\n",
    "            'FreLinear': FreTS\n",
    "        }\n",
    "        model = model_dict[self.args.model].Model(self.args).float()\n",
    "\n",
    "        if self.args.use_multi_gpu and self.args.use_gpu:\n",
    "            model = nn.DataParallel(model, device_ids=self.args.device_ids)\n",
    "        return model\n",
    "\n",
    "    def _get_data(self, flag):\n",
    "        data_set, data_loader = data_provider(self.args, flag)\n",
    "        return data_set, data_loader\n",
    "\n",
    "    def _select_optimizer(self):\n",
    "        model_optim = optim.Adam(self.model.parameters(), lr=self.args.learning_rate)\n",
    "        return model_optim\n",
    "\n",
    "    def _select_criterion(self):\n",
    "        criterion = nn.MSELoss()\n",
    "        return criterion\n",
    "\n",
    "    def vali(self, vali_data, vali_loader, criterion):\n",
    "        total_loss = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(vali_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                batch_y = batch_y.float()\n",
    "\n",
    "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "\n",
    "                # decoder input\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
    "                # encoder - decoder\n",
    "                if self.args.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if 'Linear' in self.args.model:\n",
    "                            outputs = self.model(batch_x)\n",
    "                        else:\n",
    "                            if self.args.output_attention:\n",
    "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                else:\n",
    "                    if 'Linear' in self.args.model:\n",
    "                        outputs = self.model(batch_x)\n",
    "                    else:\n",
    "                        if self.args.output_attention:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                f_dim = -1 if self.args.features == 'MS' else 0\n",
    "                outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
    "\n",
    "                pred = outputs.detach().cpu()\n",
    "                true = batch_y.detach().cpu()\n",
    "\n",
    "                loss = criterion(pred, true)\n",
    "\n",
    "                total_loss.append(loss)\n",
    "        total_loss = np.average(total_loss)\n",
    "        self.model.train()\n",
    "        return total_loss\n",
    "\n",
    "    def train(self, setting):\n",
    "        train_data, train_loader = self._get_data(flag='train')\n",
    "        if not self.args.train_only:\n",
    "            vali_data, vali_loader = self._get_data(flag='val')\n",
    "            test_data, test_loader = self._get_data(flag='test')\n",
    "\n",
    "        path = os.path.join(self.args.checkpoints, setting)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "        time_now = time.time()\n",
    "\n",
    "        train_steps = len(train_loader)\n",
    "        early_stopping = EarlyStopping(patience=self.args.patience, verbose=True)\n",
    "\n",
    "        model_optim = self._select_optimizer()\n",
    "        criterion = self._select_criterion()\n",
    "\n",
    "        total_params = 0\n",
    "        for name, parameter in self.model.named_parameters():\n",
    "            if not parameter.requires_grad: continue\n",
    "            param = parameter.numel()\n",
    "            total_params += param\n",
    "        print(f\"Total Trainable Params: {total_params}\")\n",
    "\n",
    "        if self.args.use_amp:\n",
    "            scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        for epoch in range(self.args.train_epochs):\n",
    "            iter_count = 0\n",
    "            train_loss = []\n",
    "\n",
    "            self.model.train()\n",
    "            epoch_time = time.time()\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "                iter_count += 1\n",
    "                model_optim.zero_grad()\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                batch_y = batch_y.float().to(self.device)\n",
    "\n",
    "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "\n",
    "                # decoder input\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
    "\n",
    "                # encoder - decoder\n",
    "                if self.args.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if 'Linear' in self.args.model:\n",
    "                            outputs = self.model(batch_x)\n",
    "                        else:\n",
    "                            if self.args.output_attention:\n",
    "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "                        f_dim = -1 if self.args.features == 'MS' else 0\n",
    "                        outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                        batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
    "                        loss = criterion(outputs, batch_y)\n",
    "                        train_loss.append(loss.item())\n",
    "                else:\n",
    "                    if 'Linear' in self.args.model:\n",
    "                            outputs = self.model(batch_x)\n",
    "                    else:\n",
    "                        if self.args.output_attention:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark, batch_y)\n",
    "                    # print(outputs.shape,batch_y.shape)\n",
    "                    f_dim = -1 if self.args.features == 'MS' else 0\n",
    "                    outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                    batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    train_loss.append(loss.item())\n",
    "\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "                    speed = (time.time() - time_now) / iter_count\n",
    "                    left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n",
    "                    print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                    iter_count = 0\n",
    "                    time_now = time.time()\n",
    "\n",
    "                if self.args.use_amp:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(model_optim)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    model_optim.step()\n",
    "\n",
    "            print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "            train_loss = np.average(train_loss)\n",
    "            if not self.args.train_only:\n",
    "                vali_loss = self.vali(vali_data, vali_loader, criterion)\n",
    "                test_loss = self.vali(test_data, test_loader, criterion)\n",
    "\n",
    "                print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                    epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "                early_stopping(vali_loss, self.model, path)\n",
    "            else:\n",
    "                print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f}\".format(\n",
    "                    epoch + 1, train_steps, train_loss))\n",
    "                early_stopping(train_loss, self.model, path)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "            adjust_learning_rate(model_optim, epoch + 1, self.args)\n",
    "\n",
    "        best_model_path = path + '/' + 'checkpoint.pth'\n",
    "        self.model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def test(self, setting, test=0):\n",
    "        test_data, test_loader = self._get_data(flag='test')\n",
    "\n",
    "        if test:\n",
    "            print('loading model')\n",
    "            self.model.load_state_dict(torch.load(os.path.join('./checkpoints/' + setting, 'checkpoint.pth')))\n",
    "\n",
    "        preds = []\n",
    "        trues = []\n",
    "        inputx = []\n",
    "        folder_path = './test_results/' + setting + '/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(test_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                batch_y = batch_y.float().to(self.device)\n",
    "\n",
    "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "\n",
    "                # decoder input\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
    "                # encoder - decoder\n",
    "                if self.args.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if 'Linear' in self.args.model:\n",
    "                            outputs = self.model(batch_x)\n",
    "                        else:\n",
    "                            if self.args.output_attention:\n",
    "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                else:\n",
    "                    if 'Linear' in self.args.model:\n",
    "                            outputs = self.model(batch_x)\n",
    "                    else:\n",
    "                        if self.args.output_attention:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "\n",
    "                        else:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "\n",
    "                f_dim = -1 if self.args.features == 'MS' else 0\n",
    "                # print(outputs.shape,batch_y.shape)\n",
    "                outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -self.args.pred_len:, f_dim:].to(self.device)\n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                batch_y = batch_y.detach().cpu().numpy()\n",
    "\n",
    "                pred = outputs  # outputs.detach().cpu().numpy()  # .squeeze()\n",
    "                true = batch_y  # batch_y.detach().cpu().numpy()  # .squeeze()\n",
    "\n",
    "                preds.append(pred)\n",
    "                trues.append(true)\n",
    "                inputx.append(batch_x.detach().cpu().numpy())\n",
    "                if i % 20 == 0:\n",
    "                    input = batch_x.detach().cpu().numpy()\n",
    "                    gt = np.concatenate((input[0, :, -1], true[0, :, -1]), axis=0)\n",
    "                    pd = np.concatenate((input[0, :, -1], pred[0, :, -1]), axis=0)\n",
    "                    visual(gt, pd, os.path.join(folder_path, str(i) + '.pdf'))\n",
    "\n",
    "        if self.args.test_flop:\n",
    "            test_params_flop((batch_x.shape[1],batch_x.shape[2]))\n",
    "            exit()\n",
    "        preds = np.array(preds)\n",
    "        trues = np.array(trues)\n",
    "        inputx = np.array(inputx)\n",
    "\n",
    "        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
    "        trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\n",
    "        inputx = inputx.reshape(-1, inputx.shape[-2], inputx.shape[-1])\n",
    "\n",
    "        # result save\n",
    "        folder_path = './results/' + setting + '/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        mae, mse, rmse, mape, mspe, rse, corr = metric(preds, trues)\n",
    "        print('mse:{}, mae:{}, rmse:{}'.format(mse, mae, rmse))\n",
    "        f = open(\"result.txt\", 'a')\n",
    "        f.write(setting + \"  \\n\")\n",
    "        f.write('mse:{}, mae:{}, rse:{}, rmse:{}'.format(mse, mae, rse, rmse))\n",
    "        f.write('\\n')\n",
    "        f.write('\\n')\n",
    "        f.close()\n",
    "\n",
    "        # np.save(folder_path + 'metrics.npy', np.array([mae, mse, rmse, mape, mspe,rse, corr]))\n",
    "        np.save(folder_path + 'pred.npy', preds)\n",
    "        # np.save(folder_path + 'true.npy', trues)\n",
    "        # np.save(folder_path + 'x.npy', inputx)\n",
    "        return\n",
    "\n",
    "    def predict(self, setting, load=False):\n",
    "        pred_data, pred_loader = self._get_data(flag='pred')\n",
    "\n",
    "        if load:\n",
    "            path = os.path.join(self.args.checkpoints, setting)\n",
    "            best_model_path = path + '/' + 'checkpoint.pth'\n",
    "            self.model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "        preds = []\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(pred_loader):\n",
    "                batch_x = batch_x.float().to(self.device)\n",
    "                batch_y = batch_y.float()\n",
    "                batch_x_mark = batch_x_mark.float().to(self.device)\n",
    "                batch_y_mark = batch_y_mark.float().to(self.device)\n",
    "\n",
    "                # decoder input\n",
    "                dec_inp = torch.zeros([batch_y.shape[0], self.args.pred_len, batch_y.shape[2]]).float().to(batch_y.device)\n",
    "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(self.device)\n",
    "                # encoder - decoder\n",
    "                if self.args.use_amp:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        if 'Linear' in self.args.model:\n",
    "                            outputs = self.model(batch_x)\n",
    "                        else:\n",
    "                            if self.args.output_attention:\n",
    "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                            else:\n",
    "                                outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                else:\n",
    "                    if 'Linear' in self.args.model:\n",
    "                        outputs = self.model(batch_x)\n",
    "                    else:\n",
    "                        if self.args.output_attention:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "                        else:\n",
    "                            outputs = self.model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                pred = outputs.detach().cpu().numpy()  # .squeeze()\n",
    "                preds.append(pred)\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
    "        if (pred_data.scale):\n",
    "            preds = pred_data.inverse_transform(preds)\n",
    "\n",
    "        # result save\n",
    "        folder_path = './results/' + setting + '/'\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        np.save(folder_path + 'real_prediction.npy', preds)\n",
    "        pd.DataFrame(np.append(np.transpose([pred_data.future_dates]), preds[0], axis=1), columns=pred_data.cols).to_csv(folder_path + 'real_prediction.csv', index=False)\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    is_training=1\n",
    "    train_only=False\n",
    "    model_id='ExchangeTrial96'\n",
    "    model='FreLinear'\n",
    "    data='exchange'\n",
    "    root_path='./dataset/'\n",
    "    data_path='exchange_rate.csv'\n",
    "    channel_independence=0\n",
    "    features='M'\n",
    "    target='OT'\n",
    "    freq='h'\n",
    "    checkpoints='./checkpoints/'\n",
    "    seq_len=96\n",
    "    label_len=48\n",
    "    pred_len=96\n",
    "    individual=False\n",
    "    embed_type=0\n",
    "    enc_in=7\n",
    "    dec_in=7\n",
    "    c_out=7\n",
    "    d_model=512\n",
    "    n_heads=8\n",
    "    e_layers=2\n",
    "    d_layers=1\n",
    "    d_ff=2048\n",
    "    moving_avg=25\n",
    "    factor=1\n",
    "    distil=True\n",
    "    dropout=0.05\n",
    "    embed='timeF'\n",
    "    activation='gelu'\n",
    "    output_attention=False\n",
    "    do_predict=False\n",
    "    num_workers=0\n",
    "    itr=1\n",
    "    train_epochs=500\n",
    "    batch_size=50\n",
    "    patience=3\n",
    "    learning_rate=0.0001\n",
    "    des='Exp'\n",
    "    loss='mse'\n",
    "    lradj='type1'\n",
    "    use_amp=False\n",
    "    use_gpu=True\n",
    "    gpu=0\n",
    "    use_multi_gpu=False\n",
    "    devices='0,1,2'\n",
    "    test_flop=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "<class '__main__.args'>\n"
     ]
    }
   ],
   "source": [
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "if args.use_gpu and args.use_multi_gpu:\n",
    "    args.dvices = args.devices.replace(' ', '')\n",
    "    device_ids = args.devices.split(',')\n",
    "    args.device_ids = [int(id_) for id_ in device_ids]\n",
    "    args.gpu = args.device_ids[0]\n",
    "\n",
    "print('Args in experiment:')\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : ExchangeTrial96_FreLinear_exchange_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 5120\n",
      "val 1424\n",
      "test 663\n",
      "Total Trainable Params: 3236832\n",
      "\titers: 100, epoch: 1 | loss: 0.0065862\n",
      "\tspeed: 0.0151s/iter; left time: 768.9238s\n",
      "Epoch: 1 cost time: 1.5426125526428223\n",
      "Epoch: 1, Steps: 102 | Train Loss: 0.0157684 Vali Loss: 0.0069405 Test Loss: 0.0050341\n",
      "Validation loss decreased (inf --> 0.006940).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 0.0055324\n",
      "\tspeed: 0.0187s/iter; left time: 948.7056s\n",
      "Epoch: 2 cost time: 1.5090439319610596\n",
      "Epoch: 2, Steps: 102 | Train Loss: 0.0073909 Vali Loss: 0.0070563 Test Loss: 0.0048450\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 0.0070791\n",
      "\tspeed: 0.0181s/iter; left time: 916.8483s\n",
      "Epoch: 3 cost time: 1.5088677406311035\n",
      "Epoch: 3, Steps: 102 | Train Loss: 0.0067811 Vali Loss: 0.0059217 Test Loss: 0.0046470\n",
      "Validation loss decreased (0.006940 --> 0.005922).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 0.0055328\n",
      "\tspeed: 0.0179s/iter; left time: 905.4593s\n",
      "Epoch: 4 cost time: 1.4790863990783691\n",
      "Epoch: 4, Steps: 102 | Train Loss: 0.0065945 Vali Loss: 0.0069180 Test Loss: 0.0046109\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 0.0066680\n",
      "\tspeed: 0.0175s/iter; left time: 881.9875s\n",
      "Epoch: 5 cost time: 1.4765541553497314\n",
      "Epoch: 5, Steps: 102 | Train Loss: 0.0065048 Vali Loss: 0.0063410 Test Loss: 0.0044592\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Updating learning rate to 6.25e-06\n",
      "\titers: 100, epoch: 6 | loss: 0.0046245\n",
      "\tspeed: 0.0175s/iter; left time: 879.8772s\n",
      "Epoch: 6 cost time: 1.4787909984588623\n",
      "Epoch: 6, Steps: 102 | Train Loss: 0.0064581 Vali Loss: 0.0064557 Test Loss: 0.0044692\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      ">>>>>>>testing : ExchangeTrial96_FreLinear_exchange_ftM_sl96_ll48_pl96_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 663\n",
      "mse:0.004646997433155775, mae:0.05041668191552162, rmse:0.06816888600587845\n"
     ]
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "\n",
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des, ii)\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        if not args.train_only:\n",
    "            print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "            exp.test(setting)\n",
    "\n",
    "        if args.do_predict:\n",
    "            print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "            exp.predict(setting, True)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    ii = 0\n",
    "    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(args.model_id,\n",
    "                                                                                                  args.model,\n",
    "                                                                                                  args.data,\n",
    "                                                                                                  args.features,\n",
    "                                                                                                  args.seq_len,\n",
    "                                                                                                  args.label_len,\n",
    "                                                                                                  args.pred_len,\n",
    "                                                                                                  args.d_model,\n",
    "                                                                                                  args.n_heads,\n",
    "                                                                                                  args.e_layers,\n",
    "                                                                                                  args.d_layers,\n",
    "                                                                                                  args.d_ff,\n",
    "                                                                                                  args.factor,\n",
    "                                                                                                  args.embed,\n",
    "                                                                                                  args.distil,\n",
    "                                                                                                  args.des, ii)\n",
    "\n",
    "    exp = Exp(args)  # set experiments\n",
    "\n",
    "    if args.do_predict:\n",
    "        print('>>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.predict(setting, True)\n",
    "    else:\n",
    "        print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "        exp.test(setting, test=1)\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
